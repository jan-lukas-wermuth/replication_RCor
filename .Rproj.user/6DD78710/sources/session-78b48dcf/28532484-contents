rm(list = ls())

library(mnorm)
library(DescTools)
library(data.table)
library(tsDyn)
library(doParallel)
library(doRNG)
library(foreach)
library(VGAM)

source(file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/RCode/LRV_estim.R") # Source Long-Run Variance estimation function

################################## Rank autocorrelations: Simulations ##################################
load(file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/Results/Simulations/gammas_Pois1_CIs_short.RData")

# Specify general variables that are needed in all steps
mu <- 1
MC <- 1000
alpha <- 0.1
DGPs <- c("Pois1", "Pois1_Fis")
SampleSizes <- c(50, 200, 800)
rhos <- c(0, 0.34, 0.69)

#### Step 1: IID processes (independence across elements within each process) -- Proposition 2
decision_gamma_array <- array(data = NA, dim = c(length(DGPs), length(SampleSizes), MC, length(rhos)), dimnames = list(DGPs, SampleSizes, 1:MC, rhos)) # Initialize results array

# Pois1 - distribution
# Start cluster for parallel computing
cl <- makeCluster(detectCores() - 1, type = "PSOCK")
registerDoParallel(cl)

k <- 0
for (rho in rhos){
  k <- k + 1
  for (Ti in SampleSizes){
    decision_gamma_pois1 <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      X <- rpois(Ti, lambda = mu)
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, rpois(1, lambda = mu), 1 - rho)
      }
      gamma_info <- DescTools:::.DoCount(X, Y)
      gamma <- (gamma_info$C - gamma_info$D) / (gamma_info$C + gamma_info$D)
      tau <- (gamma_info$C - gamma_info$D) / choose(Ti, 2)
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      XY_TieProb <- sum((table(X, Y)/length(X))^2)
      tie_prob <- X_TieProb + Y_TieProb - XY_TieProb      
      # Define functions that are needed in the variance estimation: Vectorize relative frequency functions
      G_XY <- Vectorize(function(x_val, y_val) (mean(X <= x_val & Y <= y_val) + mean(X <= x_val & Y < y_val) + mean(X < x_val & Y <= y_val) + mean(X < x_val & Y < y_val)) / 4)
      G_X <- Vectorize(function(x_val) (mean(X < x_val) + mean(X <= x_val)) / 2)
      G_Y <- Vectorize(function(y_val) (mean(Y < y_val) + mean(Y <= y_val)) / 2)
      x_eq_y_eq <- Vectorize(function(x_val, y_val) mean(X == x_val & Y == y_val))
      x_eq <- Vectorize(function(x_val) mean(X == x_val))
      y_eq <- Vectorize(function(y_val) mean(Y == y_val))
      # Calculate Marc's variance estimator
      G_XYXY <- G_XY(X, Y)
      G_XX <- G_X(X)
      G_YY <- G_Y(Y)
      x_eqX <- x_eq(X)
      y_eqY <- y_eq(Y)
      x_eq_y_eqXY <- x_eq_y_eq(X, Y)
      var_tau <- 4 * mean((4 * G_XYXY - 2 * (G_XX + G_YY) + 1 - tau)^2)
      var_nu <- 4 * mean((x_eqX + y_eqY - x_eq_y_eqXY - tie_prob)^2)
      var_taunu <- 4 * mean((4 * G_XYXY - 2 * (G_XX + G_YY) + 1 - tau) * (x_eqX + y_eqY - x_eq_y_eqXY - tie_prob))
      var_hat <- (var_tau + gamma^2 * var_nu + 2 * gamma * var_taunu) / (1 - tie_prob)^2
      as.numeric(data.table::between(gammas_Pois1_CIs_short[k], gamma + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti), gamma + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)))
    }
    decision_gamma_pois1_fis <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      X <- rpois(Ti, lambda = mu)
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, rpois(1, lambda = mu), 1 - rho)
      }
      gamma_info <- DescTools:::.DoCount(X, Y)
      gamma <- (gamma_info$C - gamma_info$D) / (gamma_info$C + gamma_info$D)
      gamma_fis <- atanh(gamma)
      tau <- (gamma_info$C - gamma_info$D) / choose(Ti, 2)
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      XY_TieProb <- sum((table(X, Y)/length(X))^2)
      tie_prob <- X_TieProb + Y_TieProb - XY_TieProb
      # Define functions that are needed in the variance estimation: Vectorize relative frequency functions
      G_XY <- Vectorize(function(x_val, y_val) (mean(X <= x_val & Y <= y_val) + mean(X <= x_val & Y < y_val) + mean(X < x_val & Y <= y_val) + mean(X < x_val & Y < y_val)) / 4)
      G_X <- Vectorize(function(x_val) (mean(X < x_val) + mean(X <= x_val)) / 2)
      G_Y <- Vectorize(function(y_val) (mean(Y < y_val) + mean(Y <= y_val)) / 2)
      x_eq_y_eq <- Vectorize(function(x_val, y_val) mean(X == x_val & Y == y_val))
      x_eq <- Vectorize(function(x_val) mean(X == x_val))
      y_eq <- Vectorize(function(y_val) mean(Y == y_val))
      # Calculate Marc's variance estimator
      G_XYXY <- G_XY(X, Y)
      G_XX <- G_X(X)
      G_YY <- G_Y(Y)
      x_eqX <- x_eq(X)
      y_eqY <- y_eq(Y)
      x_eq_y_eqXY <- x_eq_y_eq(X, Y)
      var_tau <- 4 * mean((4 * G_XYXY - 2 * (G_XX + G_YY) + 1 - tau)^2)
      var_nu <- 4 * mean((x_eqX + y_eqY - x_eq_y_eqXY - tie_prob)^2)
      var_taunu <- 4 * mean((4 * G_XYXY - 2 * (G_XX + G_YY) + 1 - tau) * (x_eqX + y_eqY - x_eq_y_eqXY - tie_prob))
      var_hat <- (var_tau + gamma^2 * var_nu + 2 * gamma * var_taunu) / (1 - tie_prob)^2
      as.numeric(data.table::between(gammas_Pois1_CIs_short[k], tanh(gamma_fis + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - gamma^2)), tanh(gamma_fis + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - gamma^2))))
    }
    decision_gamma_array["Pois1", as.character(Ti),,as.character(rho)] <- decision_gamma_pois1
    decision_gamma_array["Pois1_Fis", as.character(Ti),,as.character(rho)] <- decision_gamma_pois1_fis
  }
} 
stopCluster(cl)

load(file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/Results/Simulations/rhobs_Pois1_CIs_short.RData")

mu <- 1
MC <- 10
alpha <- 0.1
DGPs <- c("Pois1", "Pois1_Fis")
SampleSizes <- c(50, 200, 800)
rhos <- c(0, 0.42, 0.81)
decision_rhob_array <- array(data = NA, dim = c(length(DGPs), length(SampleSizes), MC, length(rhos)), dimnames = list(DGPs, SampleSizes, 1:MC, rhos)) # Initialize results array

cl <- makeCluster(detectCores() - 1, type = "PSOCK")
registerDoParallel(cl)

k <- 0
for (rho in rhos){
  k <- k + 1
  for (Ti in SampleSizes){
    decision_rhob_pois1 <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      X <- rpois(Ti, lambda = mu)
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, rpois(1, lambda = mu), 1 - rho)
      }
      rhob <- cor(X, Y, method = "spearman")
      spearman <- 12 * (Ti - 1) / Ti^3 * cov(X, Y, method = "spearman")
      spearman_x <- 12 * (Ti - 1) / Ti^3 * cov(X, X, method = "spearman")
      spearman_y <- 12 * (Ti - 1) / Ti^3 * cov(Y, Y, method = "spearman")
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      # Define functions that are needed in the variance estimation: Vectorize relative frequency functions
      G_XY <- Vectorize(function(x_val, y_val) (mean(X <= x_val & Y <= y_val) + mean(X <= x_val & Y < y_val) + mean(X < x_val & Y <= y_val) + mean(X < x_val & Y < y_val)) / 4)
      G_X <- Vectorize(function(x_val) (mean(X < x_val) + mean(X <= x_val)) / 2)
      G_Y <- Vectorize(function(y_val) (mean(Y < y_val) + mean(Y <= y_val)) / 2)
      g_x <- Vectorize(function(x_val) mean(G_XY(x_val, Y)))
      g_y <- Vectorize(function(y_val) mean(G_XY(X, y_val)))
      F_X <- Vectorize(function(x_val) mean(X <= x_val))
      F_X_ <- Vectorize(function(x_val) mean(X < x_val))
      F_Y <- Vectorize(function(y_val) mean(Y <= y_val))
      F_Y_ <- Vectorize(function(y_val) mean(Y < y_val))
      # Calculate Marc's variance estimator
      g_xX <- g_x(X)
      g_yY <- g_y(Y)
      G_XX <- G_X(X)
      G_YY <- G_Y(Y)
      F_XX <- F_X(X)
      F_X_X <- F_X_(X)
      F_YY <- F_Y(Y)
      F_Y_Y <- F_Y_(Y)
      # Define min functions
      min1X <- Vectorize(function(x_val) mean(pmin(F_XX, F_X(x_val))))
      min2X <- Vectorize(function(x_val) mean(pmin(F_XX, F_X_(x_val))))
      min3X <- Vectorize(function(x_val) mean(pmin(F_X_X, F_X(x_val))))
      min4X <- Vectorize(function(x_val) mean(pmin(F_X_X, F_X_(x_val))))
      min1Y <- Vectorize(function(y_val) mean(pmin(F_YY, F_Y(y_val))))
      min2Y <- Vectorize(function(y_val) mean(pmin(F_YY, F_Y_(y_val))))
      min3Y <- Vectorize(function(y_val) mean(pmin(F_Y_Y, F_Y(y_val))))
      min4Y <- Vectorize(function(y_val) mean(pmin(F_Y_Y, F_Y_(y_val))))
      # Calculate min functions
      min1XX <- min1X(X)
      min2XX <- min2X(X)
      min3XX <- min3X(X)
      min4XX <- min4X(X)
      min1YY <- min1Y(Y)
      min2YY <- min2Y(Y)
      min3YY <- min3Y(Y)
      min4YY <- min4Y(Y)
      var_rho <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman)^2)
      var_rhox <- 9 * mean((2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x)^2)
      var_rhoy <- 9 * mean((2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y)^2)
      var_rhorhox <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman) * (2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x))
      var_rhorhoy <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman) * (2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y))
      var_rhoxrhoy <- 9 * mean((2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x) * (2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y))
      var_hat <- (var_rho - spearman * (var_rhorhox / spearman_x + var_rhorhoy / spearman_y) + 0.25 * spearman^2 * (var_rhox / spearman_x^2 + var_rhoy / spearman_y^2 + 2 * var_rhoxrhoy / (spearman_x * spearman_y))) / (spearman_x * spearman_y)
      as.numeric(data.table::between(rhobs_Pois1_CIs_short[k], rhob + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti), rhob + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)))
    }
    decision_rhob_pois1_fis <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      X <- rpois(Ti, lambda = mu)
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, rpois(1, lambda = mu), 1 - rho)
      }
      rhob <- cor(X, Y, method = "spearman")
      rhob_fis <- atanh(rhob)
      spearman <- 12 * (Ti - 1) / Ti^3 * cov(X, Y, method = "spearman")
      spearman_x <- 12 * (Ti - 1) / Ti^3 * cov(X, X, method = "spearman")
      spearman_y <- 12 * (Ti - 1) / Ti^3 * cov(Y, Y, method = "spearman")
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      # Define functions that are needed in the variance estimation: Vectorize relative frequency functions
      G_XY <- Vectorize(function(x_val, y_val) (mean(X <= x_val & Y <= y_val) + mean(X <= x_val & Y < y_val) + mean(X < x_val & Y <= y_val) + mean(X < x_val & Y < y_val)) / 4)
      G_X <- Vectorize(function(x_val) (mean(X < x_val) + mean(X <= x_val)) / 2)
      G_Y <- Vectorize(function(y_val) (mean(Y < y_val) + mean(Y <= y_val)) / 2)
      g_x <- Vectorize(function(x_val) mean(G_XY(x_val, Y)))
      g_y <- Vectorize(function(y_val) mean(G_XY(X, y_val)))
      F_X <- Vectorize(function(x_val) mean(X <= x_val))
      F_X_ <- Vectorize(function(x_val) mean(X < x_val))
      F_Y <- Vectorize(function(y_val) mean(Y <= y_val))
      F_Y_ <- Vectorize(function(y_val) mean(Y < y_val))
      # Calculate Marc's variance estimator
      g_xX <- g_x(X)
      g_yY <- g_y(Y)
      G_XX <- G_X(X)
      G_YY <- G_Y(Y)
      F_XX <- F_X(X)
      F_X_X <- F_X_(X)
      F_YY <- F_Y(Y)
      F_Y_Y <- F_Y_(Y)
      # Define min functions
      min1X <- Vectorize(function(x_val) mean(pmin(F_XX, F_X(x_val))))
      min2X <- Vectorize(function(x_val) mean(pmin(F_XX, F_X_(x_val))))
      min3X <- Vectorize(function(x_val) mean(pmin(F_X_X, F_X(x_val))))
      min4X <- Vectorize(function(x_val) mean(pmin(F_X_X, F_X_(x_val))))
      min1Y <- Vectorize(function(y_val) mean(pmin(F_YY, F_Y(y_val))))
      min2Y <- Vectorize(function(y_val) mean(pmin(F_YY, F_Y_(y_val))))
      min3Y <- Vectorize(function(y_val) mean(pmin(F_Y_Y, F_Y(y_val))))
      min4Y <- Vectorize(function(y_val) mean(pmin(F_Y_Y, F_Y_(y_val))))
      # Calculate min functions
      min1XX <- min1X(X)
      min2XX <- min2X(X)
      min3XX <- min3X(X)
      min4XX <- min4X(X)
      min1YY <- min1Y(Y)
      min2YY <- min2Y(Y)
      min3YY <- min3Y(Y)
      min4YY <- min4Y(Y)
      var_rho <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman)^2)
      var_rhox <- 9 * mean((2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x)^2)
      var_rhoy <- 9 * mean((2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y)^2)
      var_rhorhox <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman) * (2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x))
      var_rhorhoy <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman) * (2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y))
      var_rhoxrhoy <- 9 * mean((2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x) * (2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y))
      var_hat <- (var_rho - spearman * (var_rhorhox / spearman_x + var_rhorhoy / spearman_y) + 0.25 * spearman^2 * (var_rhox / spearman_x^2 + var_rhoy / spearman_y^2 + 2 * var_rhoxrhoy / (spearman_x * spearman_y))) / (spearman_x * spearman_y)
      as.numeric(data.table::between(rhobs_Pois1_CIs_short[k], tanh(rhob_fis + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - rhob^2)), tanh(rhob_fis + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - rhob^2))))
    }
    decision_rhob_array["Pois1", as.character(Ti),,as.character(rho)] <- decision_rhob_pois1
    decision_rhob_array["Pois1_Fis", as.character(Ti),,as.character(rho)] <- decision_rhob_pois1_fis
  }
}
stopCluster(cl)

# Skellam Distribution
load(file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/Results/Simulations/gammas_Skellam_CIs_short.RData")

DGPs <- c("Skellam11", "Skellam11_Fis")
rhos <- c(-0.665, -0.345, 0, 0.345, 0.665)

decision_gamma_array <- array(data = NA, dim = c(length(DGPs), length(SampleSizes), MC, length(rhos)), dimnames = list(DGPs, SampleSizes, 1:MC, rhos)) # Initialize results array

cl <- makeCluster(detectCores() - 1, type = "PSOCK")
registerDoParallel(cl)
k <- 0
Start_time <- Sys.time()
for (rho in rhos){
  k <- k + 1
  for (Ti in SampleSizes){
    decision_gamma_skellam11 <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      X <- rpois(Ti, lambda = 1) - rpois(Ti, lambda = 1)
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        u <- rpois(1, lambda = 1) - rpois(1, lambda = 1)
        Y[t] <- sign(rho) * sign(X[t]) * rbinom(1, abs(X[t]), abs(rho)) + ifelse(rho >= 0, 1, -1) * sign(1 - abs(rho)) * sign(u) * rbinom(1, abs(u), abs(1 - abs(rho)))
      }
      gamma_info <- DescTools:::.DoCount(X, Y)
      gamma <- (gamma_info$C - gamma_info$D) / (gamma_info$C + gamma_info$D)
      tau <- (gamma_info$C - gamma_info$D) / choose(Ti, 2)
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      XY_TieProb <- sum((table(X, Y)/length(X))^2)
      tie_prob <- X_TieProb + Y_TieProb - XY_TieProb      # Define functions that are needed in the variance estimation: Vectorize relative frequency functions
      G_XY <- Vectorize(function(x_val, y_val) (mean(X <= x_val & Y <= y_val) + mean(X <= x_val & Y < y_val) + mean(X < x_val & Y <= y_val) + mean(X < x_val & Y < y_val)) / 4)
      G_X <- Vectorize(function(x_val) (mean(X < x_val) + mean(X <= x_val)) / 2)
      G_Y <- Vectorize(function(y_val) (mean(Y < y_val) + mean(Y <= y_val)) / 2)
      x_eq_y_eq <- Vectorize(function(x_val, y_val) mean(X == x_val & Y == y_val))
      x_eq <- Vectorize(function(x_val) mean(X == x_val))
      y_eq <- Vectorize(function(y_val) mean(Y == y_val))
      # Calculate Marc's variance estimator
      G_XYXY <- G_XY(X, Y)
      G_XX <- G_X(X)
      G_YY <- G_Y(Y)
      x_eqX <- x_eq(X)
      y_eqY <- y_eq(Y)
      x_eq_y_eqXY <- x_eq_y_eq(X, Y)
      var_tau <- 4 * mean((4 * G_XYXY - 2 * (G_XX + G_YY) + 1 - tau)^2)
      var_nu <- 4 * mean((x_eqX + y_eqY - x_eq_y_eqXY - tie_prob)^2)
      var_taunu <- 4 * mean((4 * G_XYXY - 2 * (G_XX + G_YY) + 1 - tau) * (x_eqX + y_eqY - x_eq_y_eqXY - tie_prob))
      var_hat <- (var_tau + gamma^2 * var_nu + 2 * gamma * var_taunu) / (1 - tie_prob)^2
      as.numeric(data.table::between(gammas_Skellam_CIs_short[k], gamma + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti), gamma + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)))
    }
    decision_gamma_skellam11_fis <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      X <- rpois(Ti, lambda = 1) - rpois(Ti, lambda = 1)
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        u <- rpois(1, lambda = 1) - rpois(1, lambda = 1)
        Y[t] <- sign(rho) * sign(X[t]) * rbinom(1, abs(X[t]), abs(rho)) + ifelse(rho >= 0, 1, -1) * sign(1 - abs(rho)) * sign(u) * rbinom(1, abs(u), abs(1 - abs(rho)))
      }
      gamma_info <- DescTools:::.DoCount(X, Y)
      gamma <- (gamma_info$C - gamma_info$D) / (gamma_info$C + gamma_info$D)
      gamma_fis <- atanh(gamma)
      tau <- (gamma_info$C - gamma_info$D) / choose(Ti, 2)
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      XY_TieProb <- sum((table(X, Y)/length(X))^2)
      tie_prob <- X_TieProb + Y_TieProb - XY_TieProb
      # Define functions that are needed in the variance estimation: Vectorize relative frequency functions
      G_XY <- Vectorize(function(x_val, y_val) (mean(X <= x_val & Y <= y_val) + mean(X <= x_val & Y < y_val) + mean(X < x_val & Y <= y_val) + mean(X < x_val & Y < y_val)) / 4)
      G_X <- Vectorize(function(x_val) (mean(X < x_val) + mean(X <= x_val)) / 2)
      G_Y <- Vectorize(function(y_val) (mean(Y < y_val) + mean(Y <= y_val)) / 2)
      x_eq_y_eq <- Vectorize(function(x_val, y_val) mean(X == x_val & Y == y_val))
      x_eq <- Vectorize(function(x_val) mean(X == x_val))
      y_eq <- Vectorize(function(y_val) mean(Y == y_val))
      # Calculate Marc's variance estimator
      G_XYXY <- G_XY(X, Y)
      G_XX <- G_X(X)
      G_YY <- G_Y(Y)
      x_eqX <- x_eq(X)
      y_eqY <- y_eq(Y)
      x_eq_y_eqXY <- x_eq_y_eq(X, Y)
      var_tau <- 4 * mean((4 * G_XYXY - 2 * (G_XX + G_YY) + 1 - tau)^2)
      var_nu <- 4 * mean((x_eqX + y_eqY - x_eq_y_eqXY - tie_prob)^2)
      var_taunu <- 4 * mean((4 * G_XYXY - 2 * (G_XX + G_YY) + 1 - tau) * (x_eqX + y_eqY - x_eq_y_eqXY - tie_prob))
      var_hat <- (var_tau + gamma^2 * var_nu + 2 * gamma * var_taunu) / (1 - tie_prob)^2
      as.numeric(data.table::between(gammas_Skellam_CIs_short[k], tanh(gamma_fis + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - gamma^2)), tanh(gamma_fis + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - gamma^2))))
    }
    decision_gamma_array["Skellam11", as.character(Ti),,as.character(rho)] <- decision_gamma_skellam11
    decision_gamma_array["Skellam11_Fis", as.character(Ti),,as.character(rho)] <- decision_gamma_skellam11_fis
  }
} 
End_time <- Sys.time()
End_time - Start_time
stopCluster(cl)

DGPs <- c("Skellam11", "Skellam11_Fis")
rhos <- c(-0.728, -0.365, 0, 0.365, 0.728)

load(file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/Results/Simulations/rhobs_Skellam_CIs_short.RData")
Start_time <- Sys.time()
decision_rhob_array <- array(data = NA, dim = c(length(DGPs), length(SampleSizes), MC, length(rhos)), dimnames = list(DGPs, SampleSizes, 1:MC, rhos)) # Initialize results array

cl <- makeCluster(detectCores() - 1, type = "PSOCK")
registerDoParallel(cl)

k <- 0
for (rho in rhos){
  k <- k + 1
  for (Ti in SampleSizes){
    decision_rhob_skellam <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      X <- rpois(Ti, lambda = 1) - rpois(Ti, lambda = 1)
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        u <- rpois(1, lambda = 1) - rpois(1, lambda = 1)
        Y[t] <- sign(rho) * sign(X[t]) * rbinom(1, abs(X[t]), abs(rho)) + ifelse(rho >= 0, 1, -1) * sign(1 - abs(rho)) * sign(u) * rbinom(1, abs(u), abs(1 - abs(rho)))
      }
      rhob <- cor(X, Y, method = "spearman")
      spearman <- 12 * (Ti - 1) / Ti^3 * cov(X, Y, method = "spearman")
      spearman_x <- 12 * (Ti - 1) / Ti^3 * cov(X, X, method = "spearman")
      spearman_y <- 12 * (Ti - 1) / Ti^3 * cov(Y, Y, method = "spearman")
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      # Define functions that are needed in the variance estimation: Vectorize relative frequency functions
      G_XY <- Vectorize(function(x_val, y_val) (mean(X <= x_val & Y <= y_val) + mean(X <= x_val & Y < y_val) + mean(X < x_val & Y <= y_val) + mean(X < x_val & Y < y_val)) / 4)
      G_X <- Vectorize(function(x_val) (mean(X < x_val) + mean(X <= x_val)) / 2)
      G_Y <- Vectorize(function(y_val) (mean(Y < y_val) + mean(Y <= y_val)) / 2)
      g_x <- Vectorize(function(x_val) mean(G_XY(x_val, Y)))
      g_y <- Vectorize(function(y_val) mean(G_XY(X, y_val)))
      F_X <- Vectorize(function(x_val) mean(X <= x_val))
      F_X_ <- Vectorize(function(x_val) mean(X < x_val))
      F_Y <- Vectorize(function(y_val) mean(Y <= y_val))
      F_Y_ <- Vectorize(function(y_val) mean(Y < y_val))
      # Calculate Marc's variance estimator
      g_xX <- g_x(X)
      g_yY <- g_y(Y)
      G_XX <- G_X(X)
      G_YY <- G_Y(Y)
      F_XX <- F_X(X)
      F_X_X <- F_X_(X)
      F_YY <- F_Y(Y)
      F_Y_Y <- F_Y_(Y)
      # Define min functions
      min1X <- Vectorize(function(x_val) mean(pmin(F_XX, F_X(x_val))))
      min2X <- Vectorize(function(x_val) mean(pmin(F_XX, F_X_(x_val))))
      min3X <- Vectorize(function(x_val) mean(pmin(F_X_X, F_X(x_val))))
      min4X <- Vectorize(function(x_val) mean(pmin(F_X_X, F_X_(x_val))))
      min1Y <- Vectorize(function(y_val) mean(pmin(F_YY, F_Y(y_val))))
      min2Y <- Vectorize(function(y_val) mean(pmin(F_YY, F_Y_(y_val))))
      min3Y <- Vectorize(function(y_val) mean(pmin(F_Y_Y, F_Y(y_val))))
      min4Y <- Vectorize(function(y_val) mean(pmin(F_Y_Y, F_Y_(y_val))))
      # Calculate min functions
      min1XX <- min1X(X)
      min2XX <- min2X(X)
      min3XX <- min3X(X)
      min4XX <- min4X(X)
      min1YY <- min1Y(Y)
      min2YY <- min2Y(Y)
      min3YY <- min3Y(Y)
      min4YY <- min4Y(Y)
      var_rho <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman)^2)
      var_rhox <- 9 * mean((2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x)^2)
      var_rhoy <- 9 * mean((2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y)^2)
      var_rhorhox <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman) * (2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x))
      var_rhorhoy <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman) * (2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y))
      var_rhoxrhoy <- 9 * mean((2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x) * (2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y))
      var_hat <- (var_rho - spearman * (var_rhorhox / spearman_x + var_rhorhoy / spearman_y) + 0.25 * spearman^2 * (var_rhox / spearman_x^2 + var_rhoy / spearman_y^2 + 2 * var_rhoxrhoy / (spearman_x * spearman_y))) / (spearman_x * spearman_y)
      as.numeric(data.table::between(rhobs_Skellam_CIs_short[k], rhob + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti), rhob + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)))
    }
    decision_rhob_skellam_fis <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      X <- rpois(Ti, lambda = 1) - rpois(Ti, lambda = 1)
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        u <- rpois(1, lambda = 1) - rpois(1, lambda = 1)
        Y[t] <- sign(rho) * sign(X[t]) * rbinom(1, abs(X[t]), abs(rho)) + ifelse(rho >= 0, 1, -1) * sign(1 - abs(rho)) * sign(u) * rbinom(1, abs(u), abs(1 - abs(rho)))
      }
      rhob <- cor(X, Y, method = "spearman")
      rhob_fis <- atanh(rhob)
      spearman <- 12 * (Ti - 1) / Ti^3 * cov(X, Y, method = "spearman")
      spearman_x <- 12 * (Ti - 1) / Ti^3 * cov(X, X, method = "spearman")
      spearman_y <- 12 * (Ti - 1) / Ti^3 * cov(Y, Y, method = "spearman")
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      # Define functions that are needed in the variance estimation: Vectorize relative frequency functions
      G_XY <- Vectorize(function(x_val, y_val) (mean(X <= x_val & Y <= y_val) + mean(X <= x_val & Y < y_val) + mean(X < x_val & Y <= y_val) + mean(X < x_val & Y < y_val)) / 4)
      G_X <- Vectorize(function(x_val) (mean(X < x_val) + mean(X <= x_val)) / 2)
      G_Y <- Vectorize(function(y_val) (mean(Y < y_val) + mean(Y <= y_val)) / 2)
      g_x <- Vectorize(function(x_val) mean(G_XY(x_val, Y)))
      g_y <- Vectorize(function(y_val) mean(G_XY(X, y_val)))
      F_X <- Vectorize(function(x_val) mean(X <= x_val))
      F_X_ <- Vectorize(function(x_val) mean(X < x_val))
      F_Y <- Vectorize(function(y_val) mean(Y <= y_val))
      F_Y_ <- Vectorize(function(y_val) mean(Y < y_val))
      # Calculate Marc's variance estimator
      g_xX <- g_x(X)
      g_yY <- g_y(Y)
      G_XX <- G_X(X)
      G_YY <- G_Y(Y)
      F_XX <- F_X(X)
      F_X_X <- F_X_(X)
      F_YY <- F_Y(Y)
      F_Y_Y <- F_Y_(Y)
      # Define min functions
      min1X <- Vectorize(function(x_val) mean(pmin(F_XX, F_X(x_val))))
      min2X <- Vectorize(function(x_val) mean(pmin(F_XX, F_X_(x_val))))
      min3X <- Vectorize(function(x_val) mean(pmin(F_X_X, F_X(x_val))))
      min4X <- Vectorize(function(x_val) mean(pmin(F_X_X, F_X_(x_val))))
      min1Y <- Vectorize(function(y_val) mean(pmin(F_YY, F_Y(y_val))))
      min2Y <- Vectorize(function(y_val) mean(pmin(F_YY, F_Y_(y_val))))
      min3Y <- Vectorize(function(y_val) mean(pmin(F_Y_Y, F_Y(y_val))))
      min4Y <- Vectorize(function(y_val) mean(pmin(F_Y_Y, F_Y_(y_val))))
      # Calculate min functions
      min1XX <- min1X(X)
      min2XX <- min2X(X)
      min3XX <- min3X(X)
      min4XX <- min4X(X)
      min1YY <- min1Y(Y)
      min2YY <- min2Y(Y)
      min3YY <- min3Y(Y)
      min4YY <- min4Y(Y)
      var_rho <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman)^2)
      var_rhox <- 9 * mean((2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x)^2)
      var_rhoy <- 9 * mean((2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y)^2)
      var_rhorhox <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman) * (2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x))
      var_rhorhoy <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman) * (2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y))
      var_rhoxrhoy <- 9 * mean((2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x) * (2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y))
      var_hat <- (var_rho - spearman * (var_rhorhox / spearman_x + var_rhorhoy / spearman_y) + 0.25 * spearman^2 * (var_rhox / spearman_x^2 + var_rhoy / spearman_y^2 + 2 * var_rhoxrhoy / (spearman_x * spearman_y))) / (spearman_x * spearman_y)
      as.numeric(data.table::between(rhobs_Skellam_CIs_short[k], tanh(rhob_fis + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - rhob^2)), tanh(rhob_fis + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - rhob^2))))
    }
    decision_rhob_array["Skellam11", as.character(Ti),,as.character(rho)] <- decision_rhob_skellam
    decision_rhob_array["Skellam11_Fis", as.character(Ti),,as.character(rho)] <- decision_rhob_skellam_fis
  }
}
End_time <- Sys.time()
End_time - Start_time
stopCluster(cl)

save(decision_rhob_array, file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/Results/Simulations/2Series_CIs/Rho_b_coverage_skellam.RData")


# Zipf Distribution
DGPs <- c("Zipf1", "Zipf1_Fis")
# rhos <- c(0, 0.174, 0.677)
rhos <- c(sort(-log10(seq(1,9.9999,0.0999))[-1]), log10(seq(1,9.9999,0.0999)))[91:181]
# load(file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/Results/Simulations/gammas_Zipf1_CIs_short.RData")
load(file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/Results/Simulations/gammas_Zipf1_CIs.RData")

decision_gamma_array <- array(data = NA, dim = c(length(DGPs), length(SampleSizes), MC, length(rhos)), dimnames = list(DGPs, SampleSizes, 1:MC, rhos)) # Initialize results array

cl <- makeCluster(detectCores() - 1, type = "PSOCK")
registerDoParallel(cl)
k <- 0
for (rho in rhos){
  k <- k + 1
  for (Ti in SampleSizes){
    decision_gamma_zipf1 <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      X <- VGAM::rzeta(Ti, shape = mu) - 1
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, VGAM::rzeta(1, shape = mu) - 1, 1 - rho)
      }
      gamma_info <- DescTools:::.DoCount(X, Y)
      gamma <- (gamma_info$C - gamma_info$D) / (gamma_info$C + gamma_info$D)
      tau <- (gamma_info$C - gamma_info$D) / choose(Ti, 2)
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      XY_TieProb <- sum((table(X, Y)/length(X))^2)
      tie_prob <- X_TieProb + Y_TieProb - XY_TieProb      
      # Define functions that are needed in the variance estimation: Vectorize relative frequency functions
      G_XY <- Vectorize(function(x_val, y_val) (mean(X <= x_val & Y <= y_val) + mean(X <= x_val & Y < y_val) + mean(X < x_val & Y <= y_val) + mean(X < x_val & Y < y_val)) / 4)
      G_X <- Vectorize(function(x_val) (mean(X < x_val) + mean(X <= x_val)) / 2)
      G_Y <- Vectorize(function(y_val) (mean(Y < y_val) + mean(Y <= y_val)) / 2)
      x_eq_y_eq <- Vectorize(function(x_val, y_val) mean(X == x_val & Y == y_val))
      x_eq <- Vectorize(function(x_val) mean(X == x_val))
      y_eq <- Vectorize(function(y_val) mean(Y == y_val))
      # Calculate Marc's variance estimator
      G_XYXY <- G_XY(X, Y)
      G_XX <- G_X(X)
      G_YY <- G_Y(Y)
      x_eqX <- x_eq(X)
      y_eqY <- y_eq(Y)
      x_eq_y_eqXY <- x_eq_y_eq(X, Y)
      var_tau <- 4 * mean((4 * G_XYXY - 2 * (G_XX + G_YY) + 1 - tau)^2)
      var_nu <- 4 * mean((x_eqX + y_eqY - x_eq_y_eqXY - tie_prob)^2)
      var_taunu <- 4 * mean((4 * G_XYXY - 2 * (G_XX + G_YY) + 1 - tau) * (x_eqX + y_eqY - x_eq_y_eqXY - tie_prob))
      var_hat <- (var_tau + gamma^2 * var_nu + 2 * gamma * var_taunu) / (1 - tie_prob)^2
      as.numeric(data.table::between(gammas_Zipf1[k], gamma + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti), gamma + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)))
    }
    decision_gamma_zipf1_fis <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      X <- VGAM::rzeta(Ti, shape = mu) - 1
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, VGAM::rzeta(1, shape = mu) - 1, 1 - rho)
      }
      gamma_info <- DescTools:::.DoCount(X, Y)
      gamma <- (gamma_info$C - gamma_info$D) / (gamma_info$C + gamma_info$D)
      gamma_fis <- atanh(gamma)
      tau <- (gamma_info$C - gamma_info$D) / choose(Ti, 2)
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      XY_TieProb <- sum((table(X, Y)/length(X))^2)
      tie_prob <- X_TieProb + Y_TieProb - XY_TieProb
      # Define functions that are needed in the variance estimation: Vectorize relative frequency functions
      G_XY <- Vectorize(function(x_val, y_val) (mean(X <= x_val & Y <= y_val) + mean(X <= x_val & Y < y_val) + mean(X < x_val & Y <= y_val) + mean(X < x_val & Y < y_val)) / 4)
      G_X <- Vectorize(function(x_val) (mean(X < x_val) + mean(X <= x_val)) / 2)
      G_Y <- Vectorize(function(y_val) (mean(Y < y_val) + mean(Y <= y_val)) / 2)
      x_eq_y_eq <- Vectorize(function(x_val, y_val) mean(X == x_val & Y == y_val))
      x_eq <- Vectorize(function(x_val) mean(X == x_val))
      y_eq <- Vectorize(function(y_val) mean(Y == y_val))
      # Calculate Marc's variance estimator
      G_XYXY <- G_XY(X, Y)
      G_XX <- G_X(X)
      G_YY <- G_Y(Y)
      x_eqX <- x_eq(X)
      y_eqY <- y_eq(Y)
      x_eq_y_eqXY <- x_eq_y_eq(X, Y)
      var_tau <- 4 * mean((4 * G_XYXY - 2 * (G_XX + G_YY) + 1 - tau)^2)
      var_nu <- 4 * mean((x_eqX + y_eqY - x_eq_y_eqXY - tie_prob)^2)
      var_taunu <- 4 * mean((4 * G_XYXY - 2 * (G_XX + G_YY) + 1 - tau) * (x_eqX + y_eqY - x_eq_y_eqXY - tie_prob))
      var_hat <- (var_tau + gamma^2 * var_nu + 2 * gamma * var_taunu) / (1 - tie_prob)^2
      as.numeric(data.table::between(gammas_Zipf1[k], tanh(gamma_fis + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - gamma^2)), tanh(gamma_fis + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - gamma^2))))
    }
    decision_gamma_array["Zipf1", as.character(Ti),,as.character(rho)] <- decision_gamma_zipf1
    decision_gamma_array["Zipf1_Fis", as.character(Ti),,as.character(rho)] <- decision_gamma_zipf1_fis
  }
} 
stopCluster(cl)

save(decision_gamma_array, file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/Results/Simulations/2Series_CIs/Gamma_coverage_zipf.RData")

DGPs <- c("Zipf1", "Zipf1_Fis")
# rhos <- c(0, 0.255, 0.815)
rhos <- c(sort(-log10(seq(1,9.9999,0.0999))[-1]), log10(seq(1,9.9999,0.0999)))[91:181]

# load(file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/Results/Simulations/rhobs_Zipf1_CIs_short.RData")
load(file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/Results/Simulations/rhobs_Zipf1_CIs.RData")

decision_rhob_array <- array(data = NA, dim = c(length(DGPs), length(SampleSizes), MC, length(rhos)), dimnames = list(DGPs, SampleSizes, 1:MC, rhos)) # Initialize results array

cl <- makeCluster(detectCores() - 1, type = "PSOCK")
registerDoParallel(cl)
k <- 0
for (rho in rhos){
  k <- k + 1
  print("A")
  for (Ti in SampleSizes){
    decision_rhob_zipf1 <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      X <- VGAM::rzeta(Ti, shape = mu) - 1
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, VGAM::rzeta(1, shape = mu) - 1, 1 - rho)
      }
      rhob <- cor(X, Y, method = "spearman")
      spearman <- 12 * (Ti - 1) / Ti^3 * cov(X, Y, method = "spearman")
      spearman_x <- 12 * (Ti - 1) / Ti^3 * cov(X, X, method = "spearman")
      spearman_y <- 12 * (Ti - 1) / Ti^3 * cov(Y, Y, method = "spearman")
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      # Define functions that are needed in the variance estimation: Vectorize relative frequency functions
      G_XY <- Vectorize(function(x_val, y_val) (mean(X <= x_val & Y <= y_val) + mean(X <= x_val & Y < y_val) + mean(X < x_val & Y <= y_val) + mean(X < x_val & Y < y_val)) / 4)
      G_X <- Vectorize(function(x_val) (mean(X < x_val) + mean(X <= x_val)) / 2)
      G_Y <- Vectorize(function(y_val) (mean(Y < y_val) + mean(Y <= y_val)) / 2)
      g_x <- Vectorize(function(x_val) mean(G_XY(x_val, Y)))
      g_y <- Vectorize(function(y_val) mean(G_XY(X, y_val)))
      F_X <- Vectorize(function(x_val) mean(X <= x_val))
      F_X_ <- Vectorize(function(x_val) mean(X < x_val))
      F_Y <- Vectorize(function(y_val) mean(Y <= y_val))
      F_Y_ <- Vectorize(function(y_val) mean(Y < y_val))
      # Calculate Marc's variance estimator
      g_xX <- g_x(X)
      g_yY <- g_y(Y)
      G_XX <- G_X(X)
      G_YY <- G_Y(Y)
      F_XX <- F_X(X)
      F_X_X <- F_X_(X)
      F_YY <- F_Y(Y)
      F_Y_Y <- F_Y_(Y)
      # Define min functions
      min1X <- Vectorize(function(x_val) mean(pmin(F_XX, F_X(x_val))))
      min2X <- Vectorize(function(x_val) mean(pmin(F_XX, F_X_(x_val))))
      min3X <- Vectorize(function(x_val) mean(pmin(F_X_X, F_X(x_val))))
      min4X <- Vectorize(function(x_val) mean(pmin(F_X_X, F_X_(x_val))))
      min1Y <- Vectorize(function(y_val) mean(pmin(F_YY, F_Y(y_val))))
      min2Y <- Vectorize(function(y_val) mean(pmin(F_YY, F_Y_(y_val))))
      min3Y <- Vectorize(function(y_val) mean(pmin(F_Y_Y, F_Y(y_val))))
      min4Y <- Vectorize(function(y_val) mean(pmin(F_Y_Y, F_Y_(y_val))))
      # Calculate min functions
      min1XX <- min1X(X)
      min2XX <- min2X(X)
      min3XX <- min3X(X)
      min4XX <- min4X(X)
      min1YY <- min1Y(Y)
      min2YY <- min2Y(Y)
      min3YY <- min3Y(Y)
      min4YY <- min4Y(Y)
      var_rho <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman)^2)
      var_rhox <- 9 * mean((2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x)^2)
      var_rhoy <- 9 * mean((2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y)^2)
      var_rhorhox <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman) * (2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x))
      var_rhorhoy <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman) * (2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y))
      var_rhoxrhoy <- 9 * mean((2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x) * (2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y))
      var_hat <- (var_rho - spearman * (var_rhorhox / spearman_x + var_rhorhoy / spearman_y) + 0.25 * spearman^2 * (var_rhox / spearman_x^2 + var_rhoy / spearman_y^2 + 2 * var_rhoxrhoy / (spearman_x * spearman_y))) / (spearman_x * spearman_y)
      as.numeric(data.table::between(rhobs_Zipf1[k], rhob + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti), rhob + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)))
    }
    decision_rhob_zipf1_fis <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      X <- VGAM::rzeta(Ti, shape = mu) - 1
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, VGAM::rzeta(1, shape = mu) - 1, 1 - rho)
      }
      rhob <- cor(X, Y, method = "spearman")
      rhob_fis <- atanh(rhob)
      spearman <- 12 * (Ti - 1) / Ti^3 * cov(X, Y, method = "spearman")
      spearman_x <- 12 * (Ti - 1) / Ti^3 * cov(X, X, method = "spearman")
      spearman_y <- 12 * (Ti - 1) / Ti^3 * cov(Y, Y, method = "spearman")
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      # Define functions that are needed in the variance estimation: Vectorize relative frequency functions
      G_XY <- Vectorize(function(x_val, y_val) (mean(X <= x_val & Y <= y_val) + mean(X <= x_val & Y < y_val) + mean(X < x_val & Y <= y_val) + mean(X < x_val & Y < y_val)) / 4)
      G_X <- Vectorize(function(x_val) (mean(X < x_val) + mean(X <= x_val)) / 2)
      G_Y <- Vectorize(function(y_val) (mean(Y < y_val) + mean(Y <= y_val)) / 2)
      g_x <- Vectorize(function(x_val) mean(G_XY(x_val, Y)))
      g_y <- Vectorize(function(y_val) mean(G_XY(X, y_val)))
      F_X <- Vectorize(function(x_val) mean(X <= x_val))
      F_X_ <- Vectorize(function(x_val) mean(X < x_val))
      F_Y <- Vectorize(function(y_val) mean(Y <= y_val))
      F_Y_ <- Vectorize(function(y_val) mean(Y < y_val))
      # Calculate Marc's variance estimator
      g_xX <- g_x(X)
      g_yY <- g_y(Y)
      G_XX <- G_X(X)
      G_YY <- G_Y(Y)
      F_XX <- F_X(X)
      F_X_X <- F_X_(X)
      F_YY <- F_Y(Y)
      F_Y_Y <- F_Y_(Y)
      # Define min functions
      min1X <- Vectorize(function(x_val) mean(pmin(F_XX, F_X(x_val))))
      min2X <- Vectorize(function(x_val) mean(pmin(F_XX, F_X_(x_val))))
      min3X <- Vectorize(function(x_val) mean(pmin(F_X_X, F_X(x_val))))
      min4X <- Vectorize(function(x_val) mean(pmin(F_X_X, F_X_(x_val))))
      min1Y <- Vectorize(function(y_val) mean(pmin(F_YY, F_Y(y_val))))
      min2Y <- Vectorize(function(y_val) mean(pmin(F_YY, F_Y_(y_val))))
      min3Y <- Vectorize(function(y_val) mean(pmin(F_Y_Y, F_Y(y_val))))
      min4Y <- Vectorize(function(y_val) mean(pmin(F_Y_Y, F_Y_(y_val))))
      # Calculate min functions
      min1XX <- min1X(X)
      min2XX <- min2X(X)
      min3XX <- min3X(X)
      min4XX <- min4X(X)
      min1YY <- min1Y(Y)
      min2YY <- min2Y(Y)
      min3YY <- min3Y(Y)
      min4YY <- min4Y(Y)
      var_rho <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman)^2)
      var_rhox <- 9 * mean((2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x)^2)
      var_rhoy <- 9 * mean((2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y)^2)
      var_rhorhox <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman) * (2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x))
      var_rhorhoy <- 9 * mean((4 * (g_xX + g_yY + G_XX * G_YY - G_XX - G_YY)  + 1 - spearman) * (2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y))
      var_rhoxrhoy <- 9 * mean((2 * (min1XX + min2XX + min3XX + min4XX + 2 * G_XX^2 - 4 * G_XX) + 1 - spearman_x) * (2 * (min1YY + min2YY + min3YY + min4YY + 2 * G_YY^2 - 4 * G_YY) + 1 - spearman_y))
      var_hat <- (var_rho - spearman * (var_rhorhox / spearman_x + var_rhorhoy / spearman_y) + 0.25 * spearman^2 * (var_rhox / spearman_x^2 + var_rhoy / spearman_y^2 + 2 * var_rhoxrhoy / (spearman_x * spearman_y))) / (spearman_x * spearman_y)
      as.numeric(data.table::between(rhobs_Zipf1[k], tanh(rhob_fis + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - rhob^2)), tanh(rhob_fis + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - rhob^2))))
    }
    decision_rhob_array["Zipf1", as.character(Ti),,as.character(rho)] <- decision_rhob_zipf1
    decision_rhob_array["Zipf1_Fis", as.character(Ti),,as.character(rho)] <- decision_rhob_zipf1_fis
  }
}

stopCluster(cl)

save(decision_rhob_array, file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/Results/Simulations/2Series_CIs/Rhob_coverage_zipf.RData")

################################## Rank autocorrelations: Simulations ##################################
rm(list = ls())

load(file = "/home/fuchs/agmisc/wermuth/Marc/gammas_TS_Pois1_short.RData")

source(file = "/home/fuchs/agmisc/wermuth/Marc/LRV_estim.R") # Source Long-Run Variance estimation function

# Specify general variables that are needed in all steps
mu <- 1
MC <- 1000
alpha <- 0.1
DGPs <- c("Pois1_TS", "Pois1_TS_Fis")
SampleSizes <- c(50, 200, 800)
rhos <- c(0, 0.34, 0.69)

#### Step 2: Time Series processes
decision_gamma_array <- array(data = NA, dim = c(length(DGPs), length(SampleSizes), MC, length(rhos)), dimnames = list(DGPs, SampleSizes, 1:MC, rhos)) # Initialize results array

# Start cluster for parallel computing
cl <- makeCluster(detectCores() - 1, type = "PSOCK")
registerDoParallel(cl)

k <- 0
for (rho in rhos){
  k <- k + 1
  for (Ti in SampleSizes){
    decision_gamma_pois1 <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      Xt <- rpois(1, mu)
      eps <- rpois(Ti, mu*(1-0.8))
      X <- rep(NA, Ti)
      for(t in 1:Ti){
        Xt <- rbinom(1, Xt, 0.8) + eps[t]
        X[t] <- Xt
      }
      Ut <- rpois(1, mu)
      nu <- rpois(Ti, mu*(1-0.8))
      U <- rep(NA, Ti)
      for(t in 1:Ti){
        Ut <- rbinom(1, Ut, 0.8) + nu[t]
        U[t] <- Ut
      }
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, U[t], 1 - rho)
      }
      gamma_info <- DescTools:::.DoCount(X, Y)
      gamma <- (gamma_info$C - gamma_info$D) / (gamma_info$C + gamma_info$D)
      tau <- (gamma_info$C - gamma_info$D) / choose(Ti, 2)
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      XY_TieProb <- sum((table(X, Y)/length(X))^2)
      tie_prob <- X_TieProb + Y_TieProb - XY_TieProb      
      var_hat <- Gamma_LRV(X, Y, tau, tie_prob, bandwidth = "Dehling")
      as.numeric(data.table::between(gammas_TS_Pois1[k], gamma + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti), gamma + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)))
    }
    decision_gamma_pois1_fis <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      Xt <- rpois(1, mu)
      eps <- rpois(Ti, mu*(1-0.8))
      X <- rep(NA, Ti)
      for(t in 1:Ti){
        Xt <- rbinom(1, Xt, 0.8) + eps[t]
        X[t] <- Xt
      }
      Ut <- rpois(1, mu)
      nu <- rpois(Ti, mu*(1-0.8))
      U <- rep(NA, Ti)
      for(t in 1:Ti){
        Ut <- rbinom(1, Ut, 0.8) + nu[t]
        U[t] <- Ut
      }
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, U[t], 1 - rho)
      }
      gamma_info <- DescTools:::.DoCount(X, Y)
      gamma <- (gamma_info$C - gamma_info$D) / (gamma_info$C + gamma_info$D)
      gamma_fis <- atanh(gamma)
      tau <- (gamma_info$C - gamma_info$D) / choose(Ti, 2)
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      XY_TieProb <- sum((table(X, Y)/length(X))^2)
      tie_prob <- X_TieProb + Y_TieProb - XY_TieProb
      var_hat <- Gamma_LRV(X, Y, tau, tie_prob, bandwidth = "Dehling")
      as.numeric(data.table::between(gammas_TS_Pois1[k], tanh(gamma_fis + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - gamma^2)), tanh(gamma_fis + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - gamma^2))))
    }
    decision_gamma_array["Pois1_TS", as.character(Ti),,as.character(rho)] <- decision_gamma_pois1
    decision_gamma_array["Pois1_TS_Fis", as.character(Ti),,as.character(rho)] <- decision_gamma_pois1_fis
  }
} 
stopCluster(cl)

save(decision_gamma_array, file = "/home/fuchs/agmisc/wermuth/Marc/gamma_TS_Pois1_coverage.RData")


load(file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/Results/Simulations/rhobs_TS_Pois1_short.RData")
source(file = "/Users/lukaswermuth/Documents/Dr.Wermuth/Projects/RankAutocorrelations/RCode/LRV_estim.R") # Source Long-Run Variance estimation function

# Specify general variables that are needed in all steps
mu <- 1
MC <- 10
alpha <- 0.1
DGPs <- c("Pois1_TS", "Pois1_TS_Fis")
SampleSizes <- c(50, 200, 800)
rhos <- c(0, 0.42, 0.813)
decision_rhob_array <- array(data = NA, dim = c(length(DGPs), length(SampleSizes), MC, length(rhos)), dimnames = list(DGPs, SampleSizes, 1:MC, rhos)) # Initialize results array

# Start cluster for parallel computing
cl <- makeCluster(detectCores() - 1, type = "PSOCK")
registerDoParallel(cl)

k <- 0
for (rho in rhos){
  k <- k + 1
  for (Ti in SampleSizes){
    decision_rhob_pois1 <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      Xt <- rpois(1, mu)
      eps <- rpois(Ti, mu*(1-0.8))
      X <- rep(NA, Ti)
      for(t in 1:Ti){
        Xt <- rbinom(1, Xt, 0.8) + eps[t]
        X[t] <- Xt
      }
      Ut <- rpois(1, mu)
      nu <- rpois(Ti, mu*(1-0.8))
      U <- rep(NA, Ti)
      for(t in 1:Ti){
        Ut <- rbinom(1, Ut, 0.8) + nu[t]
        U[t] <- Ut
      }
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, U[t], 1 - rho)
      }
      rhob <- cor(X, Y, method = "spearman")
      spearman <- 12 * (Ti - 1) / Ti^3 * cov(X, Y, method = "spearman")
      spearman_X <- 12 * (Ti - 1) / Ti^3 * cov(X, X, method = "spearman")
      spearman_Y <- 12 * (Ti - 1) / Ti^3 * cov(Y, Y, method = "spearman")
      var_hat <- Rhob_LRV(X, Y, spearman, spearman_X, spearman_Y, bandwidth = "Dehling")
      as.numeric(data.table::between(rhobs_TS_Pois1[k], rhob + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti), rhob + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)))
    }
    decision_rhob_pois1_fis <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      Xt <- rpois(1, mu)
      eps <- rpois(Ti, mu*(1-0.8))
      X <- rep(NA, Ti)
      for(t in 1:Ti){
        Xt <- rbinom(1, Xt, 0.8) + eps[t]
        X[t] <- Xt
      }
      Ut <- rpois(1, mu)
      nu <- rpois(Ti, mu*(1-0.8))
      U <- rep(NA, Ti)
      for(t in 1:Ti){
        Ut <- rbinom(1, Ut, 0.8) + nu[t]
        U[t] <- Ut
      }
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, U[t], 1 - rho)
      }
      rhob <- cor(X, Y, method = "spearman")
      rhob_fis <- atanh(rhob)
      spearman <- 12 * (Ti - 1) / Ti^3 * cov(X, Y, method = "spearman")
      spearman_X <- 12 * (Ti - 1) / Ti^3 * cov(X, X, method = "spearman")
      spearman_Y <- 12 * (Ti - 1) / Ti^3 * cov(Y, Y, method = "spearman")
      var_hat <- Rhob_LRV(X, Y, spearman, spearman_X, spearman_Y, bandwidth = "Dehling")
      as.numeric(data.table::between(rhobs_TS_Pois1[k], tanh(rhob_fis + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - rhob^2)), tanh(rhob_fis + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - rhob^2))))
    }
    decision_rhob_array["Pois1_TS", as.character(Ti),,as.character(rho)] <- decision_rhob_pois1
    decision_rhob_array["Pois1_TS_Fis", as.character(Ti),,as.character(rho)] <- decision_rhob_pois1_fis
  }
}

stopCluster(cl)

save(decision_rhob_array, file = "/home/fuchs/agmisc/wermuth/Marc/rhob_TS_Pois1_coverage.RData")


load(file = "/home/fuchs/agmisc/wermuth/Marc/gammas_TS_Zipf_CIs_short.RData")

source(file = "/home/fuchs/agmisc/wermuth/Marc/LRV_estim.R") # Source Long-Run Variance estimation function

# Specify general variables that are needed in all steps
MC <- 12
alpha <- 0.1
DGPs <- c("Zipf_TS", "Zipf_TS_Fis")
SampleSizes <- c(50, 200, 800)
rhos <- c(0, 0.46, 0.865)

#### Step 2: Time Series processes
decision_gamma_array <- array(data = NA, dim = c(length(DGPs), length(SampleSizes), MC, length(rhos)), dimnames = list(DGPs, SampleSizes, 1:MC, rhos)) # Initialize results array

# Start cluster for parallel computing
cl <- makeCluster(detectCores() - 1, type = "PSOCK")
registerDoParallel(cl)
Start_time <- Sys.time()
k <- 0
for (rho in rhos){
  k <- k + 1
  for (Ti in (SampleSizes + 1000)){
    decision_gamma_Zipf <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      Xt <- 1
      eps <- VGAM::rzeta(Ti, shape = 1.5)
      X <- rep(NA, Ti)
      for(t in 1:Ti){
        Xt <- rbinom(1, Xt, 0.8) + eps[t]
        X[t] <- Xt
      }
      Ut <- 1
      nu <- VGAM::rzeta(Ti, shape = 1.5)
      U <- rep(NA, Ti)
      for(t in 1:Ti){
        Ut <- rbinom(1, Ut, 0.8) + nu[t]
        U[t] <- Ut
      }
      X <- X[1001:Ti]
      U <- U[1001:Ti]
      Ti <- length(X)
      Y <- rep(NA, Ti)
      for (t in 1:(Ti)) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, U[t], 1 - rho)
      }
      gamma_info <- DescTools:::.DoCount(X, Y)
      gamma <- (gamma_info$C - gamma_info$D) / (gamma_info$C + gamma_info$D)
      tau <- (gamma_info$C - gamma_info$D) / choose(Ti, 2)
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      XY_TieProb <- sum((table(X, Y)/length(X))^2)
      tie_prob <- X_TieProb + Y_TieProb - XY_TieProb      
      var_hat <- Gamma_LRV(X, Y, tau, tie_prob, bandwidth = "Dehling")
      as.numeric(data.table::between(gammas_TS_Zipf[k], gamma + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti), gamma + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)))
    }
    decision_gamma_Zipf_fis <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      Xt <- 1
      eps <- VGAM::rzeta(Ti, shape = 1.5)
      X <- rep(NA, Ti)
      for(t in 1:Ti){
        Xt <- rbinom(1, Xt, 0.8) + eps[t]
        X[t] <- Xt
      }
      Ut <- 1
      nu <- VGAM::rzeta(Ti, shape = 1.5)
      U <- rep(NA, Ti)
      for(t in 1:Ti){
        Ut <- rbinom(1, Ut, 0.8) + nu[t]
        U[t] <- Ut
      }
      X <- X[1001:Ti]
      U <- U[1001:Ti]
      Ti <- length(X)
      Y <- rep(NA, Ti)
      for (t in 1:(Ti)) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, U[t], 1 - rho)
      }
      gamma_info <- DescTools:::.DoCount(X, Y)
      gamma <- (gamma_info$C - gamma_info$D) / (gamma_info$C + gamma_info$D)
      gamma_fis <- atanh(gamma)
      tau <- (gamma_info$C - gamma_info$D) / choose(Ti, 2)
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      XY_TieProb <- sum((table(X, Y)/length(X))^2)
      tie_prob <- X_TieProb + Y_TieProb - XY_TieProb
      var_hat <- Gamma_LRV(X, Y, tau, tie_prob, bandwidth = "Dehling")
      as.numeric(data.table::between(gammas_TS_Zipf[k], tanh(gamma_fis + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - gamma^2)), tanh(gamma_fis + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - gamma^2))))
    }
    decision_gamma_array["Zipf_TS", as.character(Ti - 1000),,as.character(rho)] <- decision_gamma_Zipf
    decision_gamma_array["Zipf_TS_Fis", as.character(Ti - 1000),,as.character(rho)] <- decision_gamma_Zipf_fis
  }
} 
stopCluster(cl)
End_time <- Sys.time()
End_time - Start_time

save(decision_gamma_array, file = "/home/fuchs/agmisc/wermuth/Marc/gamma_TS_Zipf_coverage.RData")


load(file = "/home/fuchs/agmisc/wermuth/Marc/rhobs_TS_Zipf_CIs_short.RData")
source(file = "/home/fuchs/agmisc/wermuth/Marc/LRV_estim.R") # Source Long-Run Variance estimation function

# Specify general variables that are needed in all steps
MC <- 11
alpha <- 0.1
DGPs <- c("Zipf_TS", "Zipf_TS_Fis")
SampleSizes <- c(50, 200, 800)
rhos <- c(0, 0.37, 0.81)
decision_rhob_array <- array(data = NA, dim = c(length(DGPs), length(SampleSizes), MC, length(rhos)), dimnames = list(DGPs, SampleSizes, 1:MC, rhos)) # Initialize results array

# Start cluster for parallel computing
cl <- makeCluster(detectCores() - 1, type = "PSOCK")
registerDoParallel(cl)
Start_time <- Sys.time()

k <- 0
for (rho in rhos){
  k <- k + 1
  for (Ti in (SampleSizes + 1000)){
    decision_rhob_Zipf <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      Xt <- 1
      eps <- VGAM::rzeta(Ti, shape = 1.5)
      X <- rep(NA, Ti)
      for(t in 1:Ti){
        Xt <- rbinom(1, Xt, 0.8) + eps[t]
        X[t] <- Xt
      }
      Ut <- 1
      nu <- VGAM::rzeta(Ti, shape = 1.5)
      U <- rep(NA, Ti)
      for(t in 1:Ti){
        Ut <- rbinom(1, Ut, 0.8) + nu[t]
        U[t] <- Ut
      }
      X <- X[1001:Ti]
      U <- U[1001:Ti]
      Ti <- length(X)
      Y <- rep(NA, Ti)
      for (t in 1:(Ti)) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, U[t], 1 - rho)
      }
      rhob <- cor(X, Y, method = "spearman")
      spearman <- 12 * (Ti - 1) / Ti^3 * cov(X, Y, method = "spearman")
      spearman_X <- 12 * (Ti - 1) / Ti^3 * cov(X, X, method = "spearman")
      spearman_Y <- 12 * (Ti - 1) / Ti^3 * cov(Y, Y, method = "spearman")
      var_hat <- Rhob_LRV(X, Y, spearman, spearman_X, spearman_Y, bandwidth = "Dehling")
      as.numeric(data.table::between(rhobs_TS_Zipf[k], rhob + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti), rhob + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)))
    }
    decision_rhob_Zipf_fis <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      Xt <- 1
      eps <- VGAM::rzeta(Ti, shape = 1.5)
      X <- rep(NA, Ti)
      for(t in 1:Ti){
        Xt <- rbinom(1, Xt, 0.8) + eps[t]
        X[t] <- Xt
      }
      Ut <- 1
      nu <- VGAM::rzeta(Ti, shape = 1.5)
      U <- rep(NA, Ti)
      for(t in 1:Ti){
        Ut <- rbinom(1, Ut, 0.8) + nu[t]
        U[t] <- Ut
      }
      X <- X[1001:Ti]
      U <- U[1001:Ti]
      Ti <- length(X)
      Y <- rep(NA, Ti)
      for (t in 1:(Ti)) {
        Y[t] <- rbinom(1, X[t], rho) + rbinom(1, U[t], 1 - rho)
      }
      rhob <- cor(X, Y, method = "spearman")
      rhob_fis <- atanh(rhob)
      spearman <- 12 * (Ti - 1) / Ti^3 * cov(X, Y, method = "spearman")
      spearman_X <- 12 * (Ti - 1) / Ti^3 * cov(X, X, method = "spearman")
      spearman_Y <- 12 * (Ti - 1) / Ti^3 * cov(Y, Y, method = "spearman")
      var_hat <- Rhob_LRV(X, Y, spearman, spearman_X, spearman_Y, bandwidth = "Dehling")
      as.numeric(data.table::between(rhobs_TS_Zipf[k], tanh(rhob_fis + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - rhob^2)), tanh(rhob_fis + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - rhob^2))))
    }
    decision_rhob_array["Zipf_TS", as.character(Ti - 1000),,as.character(rho)] <- decision_rhob_Zipf
    decision_rhob_array["Zipf_TS_Fis", as.character(Ti - 1000),,as.character(rho)] <- decision_rhob_Zipf_fis
  }
}

stopCluster(cl)
End_time <- Sys.time()
End_time - Start_time
save(decision_rhob_array, file = "/home/fuchs/agmisc/wermuth/Marc/rhob_TS_Zipf_coverage.RData")


load(file = "/home/fuchs/agmisc/wermuth/Marc/gammas_TS_Skellam_CIs_short.RData")

source(file = "/home/fuchs/agmisc/wermuth/Marc/LRV_estim.R") # Source Long-Run Variance estimation function

# Specify general variables that are needed in all steps
MC <- 11
alpha <- 0.1
DGPs <- c("Skellam_TS", "Skellam_TS_Fis")
SampleSizes <- c(50, 200, 800)
rhos <- c(-0.65, -0.33, 0, 0.33, 0.65)

#### Step 2: Time Series processes
decision_gamma_array <- array(data = NA, dim = c(length(DGPs), length(SampleSizes), MC, length(rhos)), dimnames = list(DGPs, SampleSizes, 1:MC, rhos)) # Initialize results array

# Start cluster for parallel computing
cl <- makeCluster(detectCores() - 1, type = "PSOCK")
registerDoParallel(cl)

k <- 0
for (rho in rhos){
  k <- k + 1
  for (Ti in (SampleSizes + 1000)){
    decision_gamma_Skellam <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      Xt <- 1
      eps <- rpois(Ti, 0.2) - rpois(Ti, 0.2)
      X <- rep(NA, Ti)
      for(t in 1:Ti){
        Xt <- sign(Xt) * rbinom(1, abs(Xt), 0.8) + eps[t]
        X[t] <- Xt
      }
      Ut <- 1
      nu <- rpois(Ti, 0.2) - rpois(Ti, 0.2)
      U <- rep(NA, Ti)
      for(t in 1:Ti){
        Ut <- sign(Ut) * rbinom(1, abs(Ut), 0.8) + nu[t]
        U[t] <- Ut
      }
      X <- X[1001:Ti]
      U <- U[1001:Ti]
      Ti <- length(X)
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- sign(rho) * sign(X[t]) * rbinom(1, abs(X[t]), abs(rho)) + ifelse(rho >= 0, 1, -1) * sign(1 - abs(rho)) * sign(U[t]) * rbinom(1, abs(U[t]), abs(1 - abs(rho)))
      }
      gamma_info <- DescTools:::.DoCount(X, Y)
      gamma <- (gamma_info$C - gamma_info$D) / (gamma_info$C + gamma_info$D)
      tau <- (gamma_info$C - gamma_info$D) / choose(Ti, 2)
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      XY_TieProb <- sum((table(X, Y)/length(X))^2)
      tie_prob <- X_TieProb + Y_TieProb - XY_TieProb      
      var_hat <- Gamma_LRV(X, Y, tau, tie_prob, bandwidth = "Dehling")
      as.numeric(data.table::between(gammas_TS_Skellam[k], gamma + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti), gamma + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)))
    }
    decision_gamma_Skellam_fis <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      Xt <- 1
      eps <- rpois(Ti, 0.2) - rpois(Ti, 0.2)
      X <- rep(NA, Ti)
      for(t in 1:Ti){
        Xt <- sign(Xt) * rbinom(1, abs(Xt), 0.8) + eps[t]
        X[t] <- Xt
      }
      Ut <- 1
      nu <- rpois(Ti, 0.2) - rpois(Ti, 0.2)
      U <- rep(NA, Ti)
      for(t in 1:Ti){
        Ut <- sign(Ut) * rbinom(1, abs(Ut), 0.8) + nu[t]
        U[t] <- Ut
      }
      X <- X[1001:Ti]
      U <- U[1001:Ti]
      Ti <- length(X)
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- sign(rho) * sign(X[t]) * rbinom(1, abs(X[t]), abs(rho)) + ifelse(rho >= 0, 1, -1) * sign(1 - abs(rho)) * sign(U[t]) * rbinom(1, abs(U[t]), abs(1 - abs(rho)))
      }
      gamma_info <- DescTools:::.DoCount(X, Y)
      gamma <- (gamma_info$C - gamma_info$D) / (gamma_info$C + gamma_info$D)
      gamma_fis <- atanh(gamma)
      tau <- (gamma_info$C - gamma_info$D) / choose(Ti, 2)
      X_TieProb <- sum((table(X)/length(X))^2)
      Y_TieProb <- sum((table(Y)/length(Y))^2)
      XY_TieProb <- sum((table(X, Y)/length(X))^2)
      tie_prob <- X_TieProb + Y_TieProb - XY_TieProb
      var_hat <- Gamma_LRV(X, Y, tau, tie_prob, bandwidth = "Dehling")
      as.numeric(data.table::between(gammas_TS_Skellam[k], tanh(gamma_fis + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - gamma^2)), tanh(gamma_fis + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - gamma^2))))
    }
    decision_gamma_array["Skellam_TS", as.character(Ti - 1000),,as.character(rho)] <- decision_gamma_Skellam
    decision_gamma_array["Skellam_TS_Fis", as.character(Ti - 1000),,as.character(rho)] <- decision_gamma_Skellam_fis
  }
} 
stopCluster(cl)

save(decision_gamma_array, file = "/home/fuchs/agmisc/wermuth/Marc/gamma_TS_Skellam_coverage.RData")


load(file = "/home/fuchs/agmisc/wermuth/Marc/rhobs_TS_Skellam_CIs_short.RData")
source(file = "/home/fuchs/agmisc/wermuth/Marc/LRV_estim.R") # Source Long-Run Variance estimation function

# Specify general variables that are needed in all steps
MC <- 11
alpha <- 0.1
DGPs <- c("Skellam_TS", "Skellam_TS_Fis")
SampleSizes <- c(50, 200, 800)
rhos <- c(-0.745, -0.37, 0, 0.37, 0.745)
decision_rhob_array <- array(data = NA, dim = c(length(DGPs), length(SampleSizes), MC, length(rhos)), dimnames = list(DGPs, SampleSizes, 1:MC, rhos)) # Initialize results array

# Start cluster for parallel computing
cl <- makeCluster(detectCores() - 1, type = "PSOCK")
registerDoParallel(cl)

k <- 0
for (rho in rhos){
  k <- k + 1
  for (Ti in (SampleSizes + 1000)){
    decision_rhob_Skellam <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      Xt <- 1
      eps <- rpois(Ti, 0.2) - rpois(Ti, 0.2)
      X <- rep(NA, Ti)
      for(t in 1:Ti){
        Xt <- sign(Xt) * rbinom(1, abs(Xt), 0.8) + eps[t]
        X[t] <- Xt
      }
      Ut <- 1
      nu <- rpois(Ti, 0.2) - rpois(Ti, 0.2)
      U <- rep(NA, Ti)
      for(t in 1:Ti){
        Ut <- sign(Ut) * rbinom(1, abs(Ut), 0.8) + nu[t]
        U[t] <- Ut
      }
      X <- X[1001:Ti]
      U <- U[1001:Ti]
      Ti <- length(X)
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- sign(rho) * sign(X[t]) * rbinom(1, abs(X[t]), abs(rho)) + ifelse(rho >= 0, 1, -1) * sign(1 - abs(rho)) * sign(U[t]) * rbinom(1, abs(U[t]), abs(1 - abs(rho)))
      }
      rhob <- cor(X, Y, method = "spearman")
      spearman <- 12 * (Ti - 1) / Ti^3 * cov(X, Y, method = "spearman")
      spearman_X <- 12 * (Ti - 1) / Ti^3 * cov(X, X, method = "spearman")
      spearman_Y <- 12 * (Ti - 1) / Ti^3 * cov(Y, Y, method = "spearman")
      var_hat <- Rhob_LRV(X, Y, spearman, spearman_X, spearman_Y, bandwidth = "Dehling")
      as.numeric(data.table::between(rhobs_TS_Skellam[k], rhob + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti), rhob + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)))
    }
    decision_rhob_Skellam_fis <- foreach(i = 1:MC, .combine = 'rbind') %dopar% {
      set.seed(i) # Is it sensible to use the same seed for all sizes? I think yes
      Xt <- 1
      eps <- rpois(Ti, 0.2) - rpois(Ti, 0.2)
      X <- rep(NA, Ti)
      for(t in 1:Ti){
        Xt <- sign(Xt) * rbinom(1, abs(Xt), 0.8) + eps[t]
        X[t] <- Xt
      }
      Ut <- 1
      nu <- rpois(Ti, 0.2) - rpois(Ti, 0.2)
      U <- rep(NA, Ti)
      for(t in 1:Ti){
        Ut <- sign(Ut) * rbinom(1, abs(Ut), 0.8) + nu[t]
        U[t] <- Ut
      }
      X <- X[1001:Ti]
      U <- U[1001:Ti]
      Ti <- length(X)
      Y <- rep(NA, Ti)
      for (t in 1:Ti) {
        Y[t] <- sign(rho) * sign(X[t]) * rbinom(1, abs(X[t]), abs(rho)) + ifelse(rho >= 0, 1, -1) * sign(1 - abs(rho)) * sign(U[t]) * rbinom(1, abs(U[t]), abs(1 - abs(rho)))
      }
      rhob <- cor(X, Y, method = "spearman")
      rhob_fis <- atanh(rhob)
      spearman <- 12 * (Ti - 1) / Ti^3 * cov(X, Y, method = "spearman")
      spearman_X <- 12 * (Ti - 1) / Ti^3 * cov(X, X, method = "spearman")
      spearman_Y <- 12 * (Ti - 1) / Ti^3 * cov(Y, Y, method = "spearman")
      var_hat <- Rhob_LRV(X, Y, spearman, spearman_X, spearman_Y, bandwidth = "Dehling")
      as.numeric(data.table::between(rhobs_TS_Skellam[k], tanh(rhob_fis + qnorm(alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - rhob^2)), tanh(rhob_fis + qnorm(1 - alpha/2)*sqrt(var_hat)/sqrt(Ti)/(1 - rhob^2))))
    }
    decision_rhob_array["Skellam_TS", as.character(Ti - 1000),,as.character(rho)] <- decision_rhob_Skellam
    decision_rhob_array["Skellam_TS_Fis", as.character(Ti - 1000),,as.character(rho)] <- decision_rhob_Skellam_fis
  }
}

stopCluster(cl)

save(decision_rhob_array, file = "/home/fuchs/agmisc/wermuth/Marc/rhob_TS_Skellam_coverage.RData")
